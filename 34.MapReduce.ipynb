{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierre Navaro - [Institut de Recherche MathÃ©matique de Rennes](https://irmar.univ-rennes1.fr) - [CNRS](http://www.cnrs.fr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Outils pour le Big Data - Pierre Nerzic ðŸ‡«ðŸ‡·](https://perso.univ-rennes1.fr/pierre.nerzic/Hadoop/)\n",
    "- [Writing an Hadoop MapReduce Program in Python - Michael G. Noll](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/)\n",
    "- [Hadoop MapReduce Framework Tutorials with Examples - Matthew Rathbone](https://blog.matthewrathbone.com/2013/01/05/a-quick-guide-to-hadoop-map-reduce-frameworks.html)\n",
    "- [Python course: Lambda, filter, reduce and map](http://www.python-course.eu/lambda.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python Map Reduce\n",
    "\n",
    "We will compute a norm with this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "V = [4,1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The `map(func, seq)` Python function applies the function func to all the elements of the sequence seq. It returns a new list with the elements changed by func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The function `reduce(func, seq)` continually applies the function func() to the sequence seq and return a single value. For example, reduce(f, [1, 2, 3, 4, 5]) calculates f(f(f(f(1,2),3),4),5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "from functools import reduce\n",
    "from math import sqrt\n",
    "\n",
    "f = lambda x: x*x   # Function applied\n",
    "L = map(f, V)       # map return a iterator\n",
    "s = reduce(add,L)   # reduce compute the sum\n",
    "sqrt(s) == sqrt(sum(map(f,V)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wordcount Example\n",
    "\n",
    "[WordCount](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0) is a simple application that counts the number of occurrences of each word in a given input set.\n",
    "\n",
    "This Python code uses the [Hadoop Streaming API](http://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) to pass data between our Map and Reduce code via Pythonâ€™s sys.stdin (standard input) and sys.stdout (standard output). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Map \n",
    "\n",
    "The following Python code read data from sys.stdin, split it into words and output a list of lines mapping words to their (intermediate) counts to sys.stdout. For every word it outputs <word> 1 tuples immediately. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%mkdir -p hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hadoop/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file hadoop/mapper.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "# input comes from standard input\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to standard output;\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print ('%s\\t%s' % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!chmod +x hadoop/mapper.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reduce \n",
    "\n",
    "The following code reads the results of mapper.py and sum the occurrences of each word to a final count, and then output its results to sys.stdout.\n",
    "Remember that Hadoop sorts map output so it is easier to count words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file hadoop/reducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import string\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input lines\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # strip punctuation\n",
    "    line = line.translate(None, string.punctuation)\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to sys.stdout\n",
    "            print ('{}\\t{}'.format(current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print ('{}\\t{}'.format(current_word, current_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!chmod +x hadoop/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Porro dolor dolore est est sit tempora quisquam. Amet non aliquam magnam eius. Labore dolore voluptatem est aliquam ut velit magnam. Tempora eius magnam quiquia quiquia dolor neque. Sed quaerat neque dolor ut neque dolore. Magnam aliquam neque magnam modi quaerat magnam neque. Sed est consectetur labore labore. Tempora numquam voluptatem neque dolore velit tempora.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lorem import paragraph\n",
    "\n",
    "p = paragraph()\n",
    "\n",
    "with open(\"hadoop/sample.txt\", \"w\") as sample:\n",
    "    sample.write(p)\n",
    "    \n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amet\t1\r\n",
      "Labore\t1\r\n",
      "Magnam\t1\r\n",
      "Porro\t1\r\n",
      "Sed\t2\r\n",
      "Tempora\t2\r\n",
      "aliquam\t3\r\n",
      "consectetur\t1\r\n",
      "dolor\t3\r\n",
      "dolore\t4\r\n",
      "eius\t2\r\n",
      "est\t4\r\n",
      "labore\t2\r\n",
      "magnam\t5\r\n",
      "modi\t1\r\n",
      "neque\t6\r\n",
      "non\t1\r\n",
      "numquam\t1\r\n",
      "quaerat\t2\r\n",
      "quiquia\t2\r\n",
      "quisquam\t1\r\n",
      "sit\t1\r\n",
      "tempora\t2\r\n",
      "ut\t2\r\n",
      "velit\t2\r\n",
      "voluptatem\t2\r\n"
     ]
    }
   ],
   "source": [
    "!cat hadoop/sample.txt | ./hadoop/mapper.py | sort | ./hadoop/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop/Makefile\n"
     ]
    }
   ],
   "source": [
    "%%file hadoop/Makefile\n",
    "HADOOP_TOOLS=/usr/local/Cellar/hadoop/2.8.0/libexec/share/hadoop/tools/lib/\n",
    "HDFS_DIR=/user/${USER}\n",
    "default:\n",
    "\techo \"coucou\"\n",
    "run_with_hadoop:\n",
    "\thadoop jar ${HADOOP_TOOLS}/hadoop-streaming-2.8.0.jar \\\n",
    "    -file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
    "    -file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
    "    -input ${HDFS_DIR}/books/* -output ${HDFS_DIR}/output-hadoop\n",
    "\n",
    "run_with_yarn:\n",
    "\tyarn jar ${HADOOP_TOOLS}/hadoop-streaming-2.8.0.jar \\\n",
    "\t-file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
    "\t-file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
    "\t-input ${HDFS_DIR}/books/* -output ${HDFS_DIR}/output-yarn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "```bash\n",
    "$ make run_with_hadoop\n",
    "\n",
    "$ make run_with_yarn\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
