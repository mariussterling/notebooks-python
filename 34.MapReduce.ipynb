{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierre Navaro - [Institut de Recherche MathÃ©matique de Rennes](https://irmar.univ-rennes1.fr) - [CNRS](http://www.cnrs.fr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Outils pour le Big Data - Pierre Nerzic ðŸ‡«ðŸ‡·](https://perso.univ-rennes1.fr/pierre.nerzic/Hadoop/)\n",
    "- [Writing an Hadoop MapReduce Program in Python - Michael G. Noll](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program reads text files and counts how often words occur. The input is text files and the output is text files, each line of which contains a word and the count of how often it occured.\n",
    "\n",
    "The [Worcount example](https://wiki.apache.org/hadoop/WordCount) is implemented in Java and it is the example of [MapReduce Tutorial](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)\n",
    "\n",
    "Python code use the [Hadoop Streaming API](http://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) to pass data between our Map and Reduce code via Pythonâ€™s sys.stdin (standard input) and sys.stdout (standard output). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map \n",
    "\n",
    "The following Python code read data from sys.stdin, split it into words and output a list of lines mapping words to their (intermediate) counts to sys.stdout. For every word it outputs <word> 1 tuples immediately. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hadoop/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file hadoop/mapper.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "# input comes from standard input\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to standard output;\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print ('%s\\t%s' % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x hadoop/mapper.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce \n",
    "\n",
    "The following code reads the results of mapper.py and sum the occurrences of each word to a final count, and then output its results to sys.stdout.\n",
    "Remember that Hadoop sorts map output so it is easier to count words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file hadoop/reducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input lines\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to sys.stdout\n",
    "            print ('{}\\t{}'.format(current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print ('{}\\t{}'.format(current_word, current_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x hadoop/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Velit modi labore adipisci neque ipsum dolore porro. Dolorem sit dolorem numquam amet neque tempora. Ut ipsum consectetur ut adipisci est numquam. Est ipsum ipsum ipsum modi. Eius tempora dolorem etincidunt.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lorem import paragraph\n",
    "\n",
    "p = paragraph()\n",
    "\n",
    "with open(\"hadoop/sample.txt\", \"w\") as sample:\n",
    "    sample.write(p)\n",
    "    \n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dolorem\t1\r\n",
      "Eius\t1\r\n",
      "Est\t1\r\n",
      "Ut\t1\r\n",
      "Velit\t1\r\n",
      "adipisci\t2\r\n",
      "amet\t1\r\n",
      "consectetur\t1\r\n",
      "dolore\t1\r\n",
      "dolorem\t2\r\n",
      "est\t1\r\n",
      "etincidunt.\t1\r\n",
      "ipsum\t5\r\n",
      "labore\t1\r\n",
      "modi\t1\r\n",
      "modi.\t1\r\n",
      "neque\t2\r\n",
      "numquam\t1\r\n",
      "numquam.\t1\r\n",
      "porro.\t1\r\n",
      "sit\t1\r\n",
      "tempora\t1\r\n",
      "tempora.\t1\r\n",
      "ut\t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat hadoop/sample.txt | ./hadoop/mapper.py | sort | ./hadoop/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makefile\n",
    "```makefile\n",
    "HADOOP_TOOLS=/usr/local/Cellar/hadoop/2.8.0/libexec/share/hadoop/tools/lib/\n",
    "HDFS_DIR=/user/${USER}\n",
    "default:\n",
    "\techo \"coucou\"\n",
    "run_with_hadoop:\n",
    "\thadoop jar ${HADOOP_TOOLS}/hadoop-streaming-2.8.0.jar \\\n",
    "\t-file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
    "\t-file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
    "\t-input ${HDFS_DIR}/books/* -output ${HDFS_DIR}/output\n",
    "    \n",
    "run_with_yarn:\n",
    "\tyarn jar ${HADOOP_TOOLS}/hadoop-streaming-2.8.0.jar \\\n",
    "\t-file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
    "\t-file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
    "\t-input ${HDFS_DIR}/books/* -output ${HDFS_DIR}/output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "```bash\n",
    "$ make run_with_hadoop\n",
    "```\n",
    "or\n",
    "```\n",
    "make run_with_yarn\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load Makefile\n",
    "\n",
    "default:\n",
    "\techo \"coucou\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
