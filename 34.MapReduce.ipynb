{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Pierre Navaro - [Institut de Recherche MathÃ©matique de Rennes](https://irmar.univ-rennes1.fr) - [CNRS](http://www.cnrs.fr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# References\n",
    "- [Outils pour le Big Data - Pierre Nerzic ðŸ‡«ðŸ‡·](https://perso.univ-rennes1.fr/pierre.nerzic/Hadoop/)\n",
    "- [Writing an Hadoop MapReduce Program in Python - Michael G. Noll](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/)\n",
    "- [Hadoop MapReduce Framework Tutorials with Examples - Matthew Rathbone](https://blog.matthewrathbone.com/2013/01/05/a-quick-guide-to-hadoop-map-reduce-frameworks.html)\n",
    "- [Python course: Lambda, filter, reduce and map](http://www.python-course.eu/lambda.php)\n",
    "- [Mastering Python for Data Science - Samir Madhavan](https://www.packtpub.com/big-data-and-business-intelligence/mastering-python-data-science)\n",
    "* [Implementing MapReduce with multiprocessing](https://pymotw.com/2/multiprocessing/mapreduce.html)\n",
    "* [Parallel MapReduce in Python in Ten Minutes](https://mikecvet.wordpress.com/2010/07/02/parallel-mapreduce-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data processing through MapReduce\n",
    "\n",
    "![MapReduce](http://mm-tom.s3.amazonaws.com/blog/MapReduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python Map Reduce\n",
    "\n",
    "We will compute a norm with this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "V = [4,1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The `map(func, seq)` Python function applies the function func to all the elements of the sequence seq. It returns a new list with the elements changed by func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The function `reduce(func, seq)` continually applies the function func() to the sequence seq and return a single value. For example, reduce(f, [1, 2, 3, 4, 5]) calculates f(f(f(f(1,2),3),4),5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "from functools import reduce\n",
    "from math import sqrt\n",
    "\n",
    "f = lambda x: x*x   # Function applied\n",
    "L = map(f, V)       # map return a iterator\n",
    "s = reduce(add,L)   # reduce compute the sum\n",
    "sqrt(s) == sqrt(sum(map(f,V)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wordcount Example\n",
    "\n",
    "[WordCount](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0) is a simple application that counts the number of occurrences of each word in a given input set.\n",
    "\n",
    "The input  and the output is text files, each line of which contains a \n",
    "\n",
    "Each mapper takes a line of text files as input and breaks it into words. It then emits a key/value pair of the word and 1 (separated by a tab). Each reducer sums the counts for each word and emits a single key/value with the word and sum.\n",
    "\n",
    "This Python code uses the [Hadoop Streaming API](http://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) to pass data between our Map and Reduce code via Pythonâ€™s sys.stdin (standard input) and sys.stdout (standard output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%mkdir -p hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Map \n",
    "\n",
    "The following Python code read data from sys.stdin, split it into words and output a list of lines mapping words to their (intermediate) counts to sys.stdout. For every word it outputs <word> 1 tuples immediately. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modi eius quaerat voluptatem sed adipisci quisquam. Modi aliquam porro est voluptatem aliquam voluptatem neque. Quisquam tempora quisquam aliquam amet consectetur labore modi. Sit adipisci quaerat ut. Quisquam dolor tempora dolore amet eius. Quaerat sit velit eius non dolor velit.\n",
      "\n",
      "Eius porro non ut ipsum aliquam. Tempora ut etincidunt velit quisquam dolor modi velit. Sit quisquam non dolorem dolor quisquam. Numquam est etincidunt quiquia consectetur porro magnam neque. Dolorem est amet sed tempora. Labore aliquam porro numquam dolorem numquam labore dolorem. Numquam non quaerat dolorem adipisci.\n",
      "\n",
      "Sed quaerat velit aliquam. Porro amet est dolore. Numquam ut sit amet dolore. Quisquam sit neque quaerat dolorem non. Est dolor ipsum dolore. Voluptatem dolorem etincidunt etincidunt. Etincidunt dolorem neque dolore labore consectetur.\n",
      "\n",
      "Porro etincidunt eius adipisci dolorem velit. Est adipisci labore dolore etincidunt. Est labore ipsum est labore quisquam quaerat. Eius adipisci dolorem magnam dolore voluptatem ut dolore. Sed sed porro non ut non. Quaerat quisquam sit dolor labore.\n"
     ]
    }
   ],
   "source": [
    "from lorem import text\n",
    "t = text()\n",
    "\n",
    "with open(\"hadoop/sample.txt\", \"w\") as sample:\n",
    "    sample.write(t)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file hadoop/mapper.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys, string\n",
    "\n",
    "# input comes from standard input\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # strip punctuation\n",
    "    line = line.translate(None,string.punctuation)\n",
    "\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to standard output;\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print ('%s\\t%s' % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!chmod +x hadoop/mapper.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reduce \n",
    "\n",
    "The following code reads the results of mapper.py and sum the occurrences of each word to a final count, and then output its results to sys.stdout.\n",
    "Remember that Hadoop sorts map output so it is easier to count words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file hadoop/reducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input lines\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to sys.stdout\n",
    "            print ('{}\\t{}'.format(current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print ('{}\\t{}'.format(current_word, current_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!chmod +x hadoop/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"./hadoop/mapper.py\", line 11, in <module>\r\n",
      "    line = line.translate(string.punctuation)\r\n",
      "ValueError: translation table must be 256 characters long\r\n",
      "None\t0\r\n"
     ]
    }
   ],
   "source": [
    "!cat hadoop/sample.txt | ./hadoop/mapper.py | sort | ./hadoop/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiprocessing version\n",
    "\n",
    "The multiprocessing Pool class provides a map function. Partition and distribute input to a user-specified function in pool of worker processes is automatic.\n",
    "\n",
    "## Convert mapper to a python function named `words`.\n",
    "\n",
    "Function takes the input text as argument and returns a list containing the sequence of key-value pairs (word, 1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def words(file):\n",
    "    \"\"\"\n",
    "    Read a text file and return a list of (word, 1) values.\n",
    "    \"\"\"\n",
    "    #print multiprocessing.current_process().name, 'reading', filename\n",
    "    output = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = line.translate(None,string.punctuation)\n",
    "            for word in line.split():\n",
    "                output.append((word, 1))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "translate() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-d4f3f5cd7499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hadoop/sample.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-5cad2426529b>\u001b[0m in \u001b[0;36mwords\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: translate() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "words('hadoop/sample.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[defaultdict](https://docs.python.org/3.6/library/collections.html#collections.defaultdict) from `collections` module \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-6c32a12a147d>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-6c32a12a147d>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    Download some books\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Deploying the MapReduce code on Hadoop\n",
    "\n",
    "Download some books\n",
    "* The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson\n",
    "* The Notebooks of Leonardo Da Vinci\n",
    "* Ulysses by James Joyce\n",
    "* The Art of War by 6th cent. B.C. Sunzi\n",
    "* The Adventures of Sherlock Holmes by Sir Arthur Conan Doyle\n",
    "* The Devilâ€™s Dictionary by Ambrose Bierce\n",
    "* Encyclopaedia Britannica, 11th Edition, Volume 4, Part 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Copy books to HDFS\n",
    "* Run the WordCount MapReduce\n",
    "\n",
    "Makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hadoop/Makefile\n"
     ]
    }
   ],
   "source": [
    "%%file hadoop/Makefile\n",
    "HADOOP_TOOLS=/usr/local/Cellar/hadoop/2.8.0/libexec/share/hadoop/tools/lib/\n",
    "HDFS_DIR=/user/${USER}\n",
    "default:\n",
    "\techo \"coucou\"\n",
    "run_with_hadoop:\n",
    "\thadoop jar ${HADOOP_TOOLS}/hadoop-streaming-2.8.0.jar \\\n",
    "    -file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
    "    -file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
    "    -input ${HDFS_DIR}/books/* -output ${HDFS_DIR}/output-hadoop\n",
    "\n",
    "run_with_yarn:\n",
    "\tyarn jar ${HADOOP_TOOLS}/hadoop-streaming-2.8.0.jar \\\n",
    "\t-file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
    "\t-file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
    "\t-input ${HDFS_DIR}/books/* -output ${HDFS_DIR}/output-yarn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "```bash\n",
    "$ make run_with_hadoop\n",
    "\n",
    "$ make run_with_yarn\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Py"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
