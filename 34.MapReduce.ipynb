{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Pierre Navaro - [Institut de Recherche MathÃ©matique de Rennes](https://irmar.univ-rennes1.fr) - [CNRS](http://www.cnrs.fr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# References\n",
    "- [Outils pour le Big Data - Pierre Nerzic ðŸ‡«ðŸ‡·](https://perso.univ-rennes1.fr/pierre.nerzic/Hadoop/)\n",
    "- [Writing an Hadoop MapReduce Program in Python - Michael G. Noll](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/)\n",
    "- [Hadoop MapReduce Framework Tutorials with Examples - Matthew Rathbone](https://blog.matthewrathbone.com/2013/01/05/a-quick-guide-to-hadoop-map-reduce-frameworks.html)\n",
    "- [Python course: Lambda, filter, reduce and map](http://www.python-course.eu/lambda.php)\n",
    "- [Mastering Python for Data Science - Samir Madhavan](https://www.packtpub.com/big-data-and-business-intelligence/mastering-python-data-science)\n",
    "* [Implementing MapReduce with multiprocessing](https://pymotw.com/2/multiprocessing/mapreduce.html)\n",
    "* [MPI4PY examples](https://github.com/jbornschein/mpi4py-examples)\n",
    "* [sharedmem package](http://rainwoodman.github.io/sharedmem/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data processing through MapReduce\n",
    "\n",
    "![MapReduce](http://mm-tom.s3.amazonaws.com/blog/MapReduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python Map Reduce\n",
    "\n",
    "We will compute a norm with this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "V = [4,1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The `map(func, seq)` Python function applies the function func to all the elements of the sequence seq. It returns a new list with the elements changed by func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The function `reduce(func, seq)` continually applies the function func() to the sequence seq and return a single value. For example, reduce(f, [1, 2, 3, 4, 5]) calculates f(f(f(f(1,2),3),4),5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "from functools import reduce\n",
    "from math import sqrt\n",
    "\n",
    "f = lambda x: x*x   # Function applied\n",
    "L = map(f, V)       # map return a iterator\n",
    "s = reduce(add,L)   # reduce compute the sum\n",
    "sqrt(s) == sqrt(sum(map(f,V)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wordcount Example\n",
    "\n",
    "[WordCount](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0) is a simple application that counts the number of occurrences of each word in a given input set.\n",
    "\n",
    "The input  and the output is text files, each line of which contains a \n",
    "\n",
    "Each mapper takes a line of text files as input and breaks it into words. It then emits a key/value pair of the word and 1 (separated by a tab). Each reducer sums the counts for each word and emits a single key/value with the word and sum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%mkdir -p hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non quaerat quisquam modi numquam sit voluptatem. Porro dolor amet numquam aliquam. Velit dolorem non ut. Porro quisquam voluptatem labore modi porro porro quaerat. Modi quiquia est aliquam sed modi. Numquam ut magnam dolor sed sit dolorem. Quiquia dolor quiquia quaerat adipisci. Quisquam quiquia sit neque numquam. Modi eius quaerat voluptatem porro adipisci numquam velit. Dolorem non ipsum magnam consectetur ut voluptatem.\n",
      "\n",
      "Modi voluptatem eius ut quaerat. Tempora etincidunt est dolore aliquam tempora sed. Aliquam amet non amet quisquam non porro. Adipisci non est consectetur eius aliquam est. Voluptatem aliquam est quisquam consectetur quaerat quisquam.\n",
      "\n",
      "Eius ut ut quaerat dolorem quisquam ut. Consectetur dolorem dolor porro. Quiquia dolore quaerat neque etincidunt dolore consectetur. Etincidunt est neque velit porro labore. Labore magnam non quisquam consectetur sit. Quisquam dolor quaerat sed. Quiquia dolore modi aliquam neque dolorem quiquia modi. Amet quaerat quiquia dolore labore neque est porro.\n",
      "\n",
      "Magnam voluptatem etincidunt consectetur tempora non. Ipsum quaerat adipisci numquam est. Voluptatem non dolorem ipsum. Etincidunt amet labore est labore. Velit est modi modi labore ut. Numquam porro labore ut. Neque dolore sed sed non adipisci. Ipsum sit dolore quisquam neque etincidunt.\n"
     ]
    }
   ],
   "source": [
    "from lorem import text\n",
    "t = text()\n",
    "\n",
    "with open(\"hadoop/sample.txt\", \"w\") as sample:\n",
    "    sample.write(t)\n",
    "\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First version\n",
    "\n",
    "## Read file and return a key/value pairs\n",
    "\n",
    "Python function with text as input and return a sorted sequence of (word, 1) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def words(file):\n",
    "    \"\"\"\n",
    "    Read a text file and return a sorted list of (word, 1) values.\n",
    "    \"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    output = []\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = line.translate(translator)\n",
    "            for word in line.split():\n",
    "                word = word.lower()\n",
    "                output.append((word, 1))\n",
    "    output.sort()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adipisci', 1),\n",
       " ('adipisci', 1),\n",
       " ('adipisci', 1),\n",
       " ('adipisci', 1),\n",
       " ('adipisci', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('aliquam', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('amet', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('consectetur', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolor', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolore', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('dolorem', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('eius', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('est', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('etincidunt', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('ipsum', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('labore', 1),\n",
       " ('magnam', 1),\n",
       " ('magnam', 1),\n",
       " ('magnam', 1),\n",
       " ('magnam', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('modi', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('neque', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('non', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('numquam', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('porro', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quaerat', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quiquia', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('quisquam', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sed', 1),\n",
       " ('sit', 1),\n",
       " ('sit', 1),\n",
       " ('sit', 1),\n",
       " ('sit', 1),\n",
       " ('sit', 1),\n",
       " ('tempora', 1),\n",
       " ('tempora', 1),\n",
       " ('tempora', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('ut', 1),\n",
       " ('velit', 1),\n",
       " ('velit', 1),\n",
       " ('velit', 1),\n",
       " ('velit', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1),\n",
       " ('voluptatem', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words('hadoop/sample.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reduce \n",
    "\n",
    "The following code reads the results of words and sum the occurrences of each word to a final count, and then output the results\n",
    "as a list of (word, occurences). Two steps:\n",
    "- Group (word, 1) pairs into a dictionary as\n",
    "```python\n",
    "{word1 : [1, 1], word2 : [1, 1, 1], word3 : [1] }\n",
    "```\n",
    "- Reduce operation prints out the word and its number of occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def reduce(words):\n",
    "    \"\"\" Read the sorted list from map and print out every word with \n",
    "    its number of occurences\"\"\"\n",
    "    d = {}\n",
    "    for w in words:\n",
    "        try:\n",
    "            d[w[0]].append(w[1])\n",
    "        except KeyError:\n",
    "            d[w[0]] = [w[1]]\n",
    "    for key, value in d.items():\n",
    "        print(\"{}\\t{}\".format(key,len(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adipisci\t5\n",
      "aliquam\t7\n",
      "amet\t5\n",
      "consectetur\t7\n",
      "dolor\t5\n",
      "dolore\t7\n",
      "dolorem\t7\n",
      "eius\t4\n",
      "est\t10\n",
      "etincidunt\t6\n",
      "ipsum\t4\n",
      "labore\t8\n",
      "magnam\t4\n",
      "modi\t10\n",
      "neque\t7\n",
      "non\t10\n",
      "numquam\t7\n",
      "porro\t10\n",
      "quaerat\t11\n",
      "quiquia\t8\n",
      "quisquam\t10\n",
      "sed\t6\n",
      "sit\t5\n",
      "tempora\t3\n",
      "ut\t9\n",
      "velit\t4\n",
      "voluptatem\t8\n"
     ]
    }
   ],
   "source": [
    "reduce(words('hadoop/sample.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiprocessing version\n",
    "\n",
    "The multiprocessing Pool class provides a map function. Partition and distribute input to a user-specified function in pool of worker processes is automatic.\n",
    "\n",
    "## Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 3, 3, 6, 3, 9, 5, 7, 7, 3, 2, 7, 8, 4, 4, 7, 2, 5, 8, 6, 4, 3, 4, 8, 3, 4, 5, 3, 8, 5, 7, 6, 8, 7, 6, 7, 6, 7, 4, 5, 6, 5, 7, 5, 4]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from lorem import paragraph\n",
    "\n",
    "words = paragraph().split() # Create a list of words\n",
    "if __name__ == '__main__': # Executed only on main process.\n",
    "    with Pool(4) as p:\n",
    "        print(p.map(len, words)) # Print words length\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Pool() launches one slave process per physical processor on the computer. \n",
    "- pool.map(...) divides the input list into chunks and puts the tasks (function + chunk) on a queue.\n",
    "- Each slave process takes a task (function + a chunk of data), runs map(function, chunk), and puts the result on a result list.\n",
    "- pool.map on the master process waits until all tasks are handled and returns the concatenation of the result lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Map\n",
    "\n",
    "Same function words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "def words_mp(file):\n",
    "    \"\"\"\n",
    "    Read a text file and return a sorted list of (word, 1) values.\n",
    "    \"\"\"\n",
    "    print(mp.current_process().name, 'reading', file)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    output = []\n",
    "    try:\n",
    "        with open(file) as f:\n",
    "            for line in f:   \n",
    "                line = line.strip()\n",
    "                line = line.translate(translator)\n",
    "                for word in line.split():\n",
    "                    if word.isalpha():\n",
    "                        word = word.lower()\n",
    "                        output.append((word, 1))\n",
    "                        \n",
    "    except UnicodeDecodeError as err:\n",
    "        print(\"Some error occurred decoding file %s: %s\" % (file, err))\n",
    "                \n",
    "    output.sort()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Partition\n",
    "\n",
    "Before **reduce** operation, data must be aligned in a container. Create a function named `partition` that stores the key/value pairs from `words` into a [defaultdict](https://docs.python.org/3.6/library/collections.html#collections.defaultdict) from `collections` module. Ouput will be:\n",
    "```python\n",
    "[('word1', [1, 1]), ('word2', [1]), ('word3', [1, 1, 1])]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def partition_mp(mapped_values):\n",
    "    \"\"\"\n",
    "        Organize the mapped values by their key.\n",
    "        Returns an unsorted sequence of tuples with a key and a sequence of values.\n",
    "    \"\"\"\n",
    "    partitioned_data = collections.defaultdict(list)\n",
    "    for key, value in mapped_values:\n",
    "        partitioned_data[key].append(value)\n",
    "    return partitioned_data.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mp(item):\n",
    "    \"\"\"Convert the partitioned data for a word to a\n",
    "    tuple containing the word and the number of occurances.\n",
    "    \"\"\"\n",
    "    word, occurances = item\n",
    "    return (word, sum(occurances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainProcess reading hadoop/sample.txt\n",
      "('adipisci', 5) ('aliquam', 7) ('amet', 5) ('consectetur', 7) ('dolor', 5) ('dolore', 7) ('dolorem', 7) ('eius', 4) ('est', 10) ('etincidunt', 6) ('ipsum', 4) ('labore', 8) ('magnam', 4) ('modi', 10) ('neque', 7) ('non', 10) ('numquam', 7) ('porro', 10) ('quaerat', 11) ('quiquia', 8) ('quisquam', 10) ('sed', 6) ('sit', 5) ('tempora', 3) ('ut', 9) ('velit', 4) ('voluptatem', 8)\n"
     ]
    }
   ],
   "source": [
    "mapped_values = words_mp('hadoop/sample.txt')\n",
    "partioned_values = partition_mp(mapped_values)\n",
    "print(*map(reduce_mp,partioned_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForkPoolWorker-27 reading hadoop/books/4300-0.txt\n",
      "ForkPoolWorker-26 reading hadoop/books/20417.txt\n",
      "ForkPoolWorker-25 reading hadoop/books/19699.txt\n",
      "ForkPoolWorker-23 reading hadoop/books/132.txt\n",
      "ForkPoolWorker-24 reading hadoop/books/1661.txt\n",
      "ForkPoolWorker-28 reading hadoop/books/972.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 66482),\n",
       " ('of', 38941),\n",
       " ('and', 26981),\n",
       " ('a', 21549),\n",
       " ('in', 20797),\n",
       " ('to', 20138),\n",
       " ('is', 10447),\n",
       " ('that', 8742),\n",
       " ('he', 8552),\n",
       " ('it', 8321),\n",
       " ('was', 7786),\n",
       " ('his', 7621),\n",
       " ('with', 7145),\n",
       " ('by', 6393),\n",
       " ('for', 6350),\n",
       " ('i', 6221),\n",
       " ('on', 6192),\n",
       " ('as', 5764),\n",
       " ('at', 4981),\n",
       " ('which', 4685),\n",
       " ('be', 4584),\n",
       " ('from', 4568),\n",
       " ('are', 4432),\n",
       " ('or', 4244),\n",
       " ('you', 4189),\n",
       " ('not', 3551),\n",
       " ('but', 3516),\n",
       " ('have', 3249),\n",
       " ('this', 3226),\n",
       " ('an', 3211),\n",
       " ('all', 2962),\n",
       " ('had', 2717),\n",
       " ('him', 2685),\n",
       " ('they', 2668),\n",
       " ('her', 2583),\n",
       " ('there', 2557),\n",
       " ('one', 2528),\n",
       " ('were', 2344),\n",
       " ('we', 2286),\n",
       " ('their', 2069),\n",
       " ('said', 2061),\n",
       " ('has', 2060),\n",
       " ('my', 2023),\n",
       " ('when', 1995),\n",
       " ('so', 1972),\n",
       " ('no', 1930),\n",
       " ('its', 1905),\n",
       " ('out', 1840),\n",
       " ('been', 1804),\n",
       " ('she', 1769),\n",
       " ('if', 1720),\n",
       " ('up', 1694),\n",
       " ('who', 1685),\n",
       " ('what', 1674),\n",
       " ('me', 1672),\n",
       " ('into', 1524),\n",
       " ('them', 1517),\n",
       " ('other', 1491),\n",
       " ('some', 1489),\n",
       " ('then', 1478),\n",
       " ('about', 1446),\n",
       " ('more', 1425),\n",
       " ('may', 1409),\n",
       " ('two', 1382),\n",
       " ('will', 1356),\n",
       " ('very', 1310),\n",
       " ('your', 1232),\n",
       " ('like', 1212),\n",
       " ('man', 1208),\n",
       " ('would', 1203),\n",
       " ('see', 1161),\n",
       " ('now', 1136),\n",
       " ('these', 1135),\n",
       " ('time', 1125),\n",
       " ('only', 1123),\n",
       " ('than', 1107),\n",
       " ('first', 1099),\n",
       " ('do', 1099),\n",
       " ('after', 1096),\n",
       " ('great', 1088),\n",
       " ('mr', 1074),\n",
       " ('any', 1074),\n",
       " ('over', 1044),\n",
       " ('our', 1006),\n",
       " ('upon', 997),\n",
       " ('made', 972),\n",
       " ('must', 961),\n",
       " ('can', 948),\n",
       " ('down', 944),\n",
       " ('bloom', 931),\n",
       " ('being', 904),\n",
       " ('most', 891),\n",
       " ('n', 885),\n",
       " ('where', 876),\n",
       " ('work', 814),\n",
       " ('such', 814),\n",
       " ('could', 814),\n",
       " ('old', 805),\n",
       " ('little', 805),\n",
       " ('also', 805),\n",
       " ('under', 804),\n",
       " ('before', 766),\n",
       " ('those', 761),\n",
       " ('way', 750),\n",
       " ('should', 746),\n",
       " ('many', 744),\n",
       " ('us', 743),\n",
       " ('long', 725),\n",
       " ('much', 717),\n",
       " ('through', 714),\n",
       " ('says', 703),\n",
       " ('know', 700),\n",
       " ('hand', 700),\n",
       " ('life', 699),\n",
       " ('did', 699),\n",
       " ('between', 691),\n",
       " ('same', 690),\n",
       " ('well', 686),\n",
       " ('too', 685),\n",
       " ('new', 684),\n",
       " ('good', 684),\n",
       " ('back', 683),\n",
       " ('here', 654),\n",
       " ('part', 626),\n",
       " ('off', 620),\n",
       " ('without', 619),\n",
       " ('own', 617),\n",
       " ('years', 612),\n",
       " ('how', 606),\n",
       " ('three', 593),\n",
       " ('himself', 589),\n",
       " ('come', 577),\n",
       " ('each', 573),\n",
       " ('right', 572),\n",
       " ('found', 555),\n",
       " ('used', 553),\n",
       " ('day', 553),\n",
       " ('still', 552),\n",
       " ('make', 552),\n",
       " ('say', 549),\n",
       " ('sir', 546),\n",
       " ('project', 546),\n",
       " ('name', 543),\n",
       " ('sun', 541),\n",
       " ('though', 537),\n",
       " ('ft', 526),\n",
       " ('while', 522),\n",
       " ('another', 518),\n",
       " ('stephen', 516),\n",
       " ('water', 512),\n",
       " ('never', 507),\n",
       " ('against', 505),\n",
       " ('every', 497),\n",
       " ('men', 495),\n",
       " ('even', 493),\n",
       " ('just', 492),\n",
       " ('however', 491),\n",
       " ('again', 485),\n",
       " ('came', 484),\n",
       " ('small', 482),\n",
       " ('last', 479),\n",
       " ('eyes', 471),\n",
       " ('round', 470),\n",
       " ('light', 470),\n",
       " ('having', 470),\n",
       " ('holmes', 469),\n",
       " ('c', 467),\n",
       " ('away', 467),\n",
       " ('house', 465),\n",
       " ('general', 465),\n",
       " ('form', 465),\n",
       " ('called', 462),\n",
       " ('face', 460),\n",
       " ('go', 458),\n",
       " ('father', 452),\n",
       " ('left', 450),\n",
       " ('large', 448),\n",
       " ('young', 445),\n",
       " ('place', 445),\n",
       " ('matter', 445),\n",
       " ('head', 443),\n",
       " ('illustration', 442),\n",
       " ('side', 436),\n",
       " ('case', 430),\n",
       " ('street', 426),\n",
       " ('known', 425),\n",
       " ('give', 425),\n",
       " ('get', 424),\n",
       " ('both', 424),\n",
       " ('yes', 420),\n",
       " ('use', 420),\n",
       " ('de', 420),\n",
       " ('bridge', 419),\n",
       " ('might', 412),\n",
       " ('course', 412),\n",
       " ('works', 410),\n",
       " ('took', 410),\n",
       " ('put', 409),\n",
       " ('let', 409),\n",
       " ('british', 403),\n",
       " ('think', 402),\n",
       " ('because', 402),\n",
       " ('once', 399),\n",
       " ('am', 399),\n",
       " ('high', 397),\n",
       " ('night', 396),\n",
       " ('take', 393),\n",
       " ('john', 392),\n",
       " ('end', 392),\n",
       " ('always', 390),\n",
       " ('shall', 385),\n",
       " ('number', 384),\n",
       " ('o', 383),\n",
       " ('london', 381),\n",
       " ('among', 380),\n",
       " ('whole', 377),\n",
       " ('world', 372),\n",
       " ('year', 370),\n",
       " ('near', 366),\n",
       " ('few', 366),\n",
       " ('j', 365),\n",
       " ('town', 357),\n",
       " ('far', 357),\n",
       " ('thus', 353),\n",
       " ('earth', 351),\n",
       " ('m', 347),\n",
       " ('fig', 346),\n",
       " ('yet', 342),\n",
       " ('means', 342),\n",
       " ('something', 338),\n",
       " ('nothing', 337),\n",
       " ('gutenbergtm', 336),\n",
       " ('during', 336),\n",
       " ('country', 336),\n",
       " ('century', 336),\n",
       " ('lord', 334),\n",
       " ('certain', 334),\n",
       " ('king', 332),\n",
       " ('four', 332),\n",
       " ('does', 332),\n",
       " ('went', 330),\n",
       " ('ever', 328),\n",
       " ('body', 328),\n",
       " ('white', 327),\n",
       " ('word', 326),\n",
       " ('less', 325),\n",
       " ('given', 325),\n",
       " ('asked', 324),\n",
       " ('second', 323),\n",
       " ('open', 323),\n",
       " ('became', 322),\n",
       " ('why', 321),\n",
       " ('order', 317),\n",
       " ('until', 315),\n",
       " ('saw', 315),\n",
       " ('english', 315),\n",
       " ('death', 315),\n",
       " ('war', 314),\n",
       " ('thing', 314),\n",
       " ('look', 314),\n",
       " ('son', 313),\n",
       " ('perhaps', 313),\n",
       " ('hands', 313),\n",
       " ('times', 312),\n",
       " ('point', 311),\n",
       " ('above', 311),\n",
       " ('seen', 310),\n",
       " ('tell', 307),\n",
       " ('mind', 307),\n",
       " ('line', 306),\n",
       " ('quite', 305),\n",
       " ('city', 305),\n",
       " ('going', 304),\n",
       " ('god', 303),\n",
       " ('words', 303),\n",
       " ('kind', 303),\n",
       " ('state', 302),\n",
       " ('thought', 301),\n",
       " ('south', 300),\n",
       " ('often', 300),\n",
       " ('half', 300),\n",
       " ('home', 299),\n",
       " ('door', 297),\n",
       " ('england', 296),\n",
       " ('others', 294),\n",
       " ('five', 293),\n",
       " ('army', 292),\n",
       " ('find', 291),\n",
       " ('st', 290),\n",
       " ('history', 288),\n",
       " ('got', 288),\n",
       " ('best', 288),\n",
       " ('set', 287),\n",
       " ('sea', 286),\n",
       " ('within', 286),\n",
       " ('better', 285),\n",
       " ('woman', 284),\n",
       " ('different', 284),\n",
       " ('together', 283),\n",
       " ('important', 283),\n",
       " ('public', 282),\n",
       " ('power', 282),\n",
       " ('air', 282),\n",
       " ('full', 281),\n",
       " ('fact', 279),\n",
       " ('animals', 279),\n",
       " ('heard', 278),\n",
       " ('days', 277),\n",
       " ('cannot', 277),\n",
       " ('present', 276),\n",
       " ('later', 276),\n",
       " ('nature', 275),\n",
       " ('gave', 273),\n",
       " ('ground', 272),\n",
       " ('probably', 271),\n",
       " ('north', 270),\n",
       " ('various', 269),\n",
       " ('taken', 269),\n",
       " ('several', 269),\n",
       " ('wall', 265),\n",
       " ('dead', 265),\n",
       " ('towards', 264),\n",
       " ('mrs', 263),\n",
       " ('land', 263),\n",
       " ('brought', 263),\n",
       " ('black', 262),\n",
       " ('enemy', 261),\n",
       " ('early', 261),\n",
       " ('whose', 260),\n",
       " ('along', 259),\n",
       " ('church', 258),\n",
       " ('s', 257),\n",
       " ('miss', 256),\n",
       " ('behind', 256),\n",
       " ('poor', 255),\n",
       " ('till', 253),\n",
       " ('whom', 251),\n",
       " ('passed', 251),\n",
       " ('building', 250),\n",
       " ('things', 250),\n",
       " ('states', 249),\n",
       " ('system', 248),\n",
       " ('p', 247),\n",
       " ('enough', 247),\n",
       " ('told', 246),\n",
       " ('river', 244),\n",
       " ('red', 244),\n",
       " ('period', 244),\n",
       " ('turned', 241),\n",
       " ('sometimes', 241),\n",
       " ('morning', 241),\n",
       " ('following', 240),\n",
       " ('possible', 239),\n",
       " ('held', 238),\n",
       " ('money', 237),\n",
       " ('short', 235),\n",
       " ('modern', 235),\n",
       " ('born', 235),\n",
       " ('per', 234),\n",
       " ('itself', 234),\n",
       " ('government', 234),\n",
       " ('age', 233),\n",
       " ('read', 232),\n",
       " ('natural', 232),\n",
       " ('almost', 232),\n",
       " ('died', 231),\n",
       " ('love', 230),\n",
       " ('forms', 229),\n",
       " ('call', 229),\n",
       " ('people', 228),\n",
       " ('common', 228),\n",
       " ('wife', 227),\n",
       " ('position', 227),\n",
       " ('b', 226),\n",
       " ('true', 225),\n",
       " ('rather', 225),\n",
       " ('professor', 225),\n",
       " ('voice', 223),\n",
       " ('since', 222),\n",
       " ('chief', 222),\n",
       " ('energy', 221),\n",
       " ('west', 220),\n",
       " ('stone', 220),\n",
       " ('united', 219),\n",
       " ('done', 219),\n",
       " ('ancient', 219),\n",
       " ('roman', 218),\n",
       " ('dark', 218),\n",
       " ('human', 217),\n",
       " ('heart', 217),\n",
       " ('free', 217),\n",
       " ('hat', 216),\n",
       " ('moment', 215),\n",
       " ('iron', 215),\n",
       " ('either', 213),\n",
       " ('fine', 212),\n",
       " ('next', 212),\n",
       " ('french', 212),\n",
       " ('room', 211),\n",
       " ('cases', 211),\n",
       " ('act', 210),\n",
       " ('main', 209),\n",
       " ('east', 208),\n",
       " ('load', 207),\n",
       " ('soon', 207),\n",
       " ('mouth', 207),\n",
       " ('railway', 206),\n",
       " ('society', 205),\n",
       " ('want', 205),\n",
       " ('latter', 205),\n",
       " ('hear', 204),\n",
       " ('parts', 203),\n",
       " ('knew', 203),\n",
       " ('built', 203),\n",
       " ('bricks', 202),\n",
       " ('law', 202),\n",
       " ('ten', 201),\n",
       " ('office', 201),\n",
       " ('trade', 200),\n",
       " ('show', 200),\n",
       " ('according', 199),\n",
       " ('evolution', 198),\n",
       " ('surface', 198),\n",
       " ('person', 198),\n",
       " ('hair', 198),\n",
       " ('account', 198),\n",
       " ('living', 197),\n",
       " ('least', 197),\n",
       " ('terms', 196),\n",
       " ('seems', 196),\n",
       " ('become', 196),\n",
       " ('mother', 195),\n",
       " ('making', 195),\n",
       " ('began', 195),\n",
       " ('fire', 194),\n",
       " ('eye', 194),\n",
       " ('strong', 193),\n",
       " ('looked', 193),\n",
       " ('keep', 193),\n",
       " ('business', 193),\n",
       " ('followed', 191),\n",
       " ('doubt', 191),\n",
       " ('tzu', 190),\n",
       " ('members', 190),\n",
       " ('book', 190),\n",
       " ('animal', 190),\n",
       " ('already', 190),\n",
       " ('paris', 189),\n",
       " ('royal', 189),\n",
       " ('paper', 189),\n",
       " ('lines', 189),\n",
       " ('generally', 188),\n",
       " ('art', 188),\n",
       " ('photo', 187),\n",
       " ('fellow', 187),\n",
       " ('themselves', 187),\n",
       " ('gold', 187),\n",
       " ('points', 186),\n",
       " ('across', 186),\n",
       " ('six', 185),\n",
       " ('usually', 184),\n",
       " ('especially', 183),\n",
       " ('anything', 183),\n",
       " ('whether', 182),\n",
       " ('value', 182),\n",
       " ('lost', 182),\n",
       " ('gone', 182),\n",
       " ('coming', 182),\n",
       " ('gutenberg', 181),\n",
       " ('due', 181),\n",
       " ('question', 178),\n",
       " ('carried', 178),\n",
       " ('henry', 177),\n",
       " ('friend', 177),\n",
       " ('past', 176),\n",
       " ('strength', 176),\n",
       " ('sent', 176),\n",
       " ('turn', 175),\n",
       " ('hard', 175),\n",
       " ('foundation', 175),\n",
       " ('court', 175),\n",
       " ('lady', 174),\n",
       " ('looking', 174),\n",
       " ('dr', 174),\n",
       " ('bed', 173),\n",
       " ('led', 173),\n",
       " ('cried', 173),\n",
       " ('big', 172),\n",
       " ('family', 171),\n",
       " ('principal', 170),\n",
       " ('published', 169),\n",
       " ('laid', 169),\n",
       " ('interest', 169),\n",
       " ('force', 169),\n",
       " ('except', 169),\n",
       " ('green', 168),\n",
       " ('britain', 168),\n",
       " ('lower', 168),\n",
       " ('formed', 168),\n",
       " ('centre', 167),\n",
       " ('span', 166),\n",
       " ('sure', 166),\n",
       " ('electronic', 165),\n",
       " ('considerable', 165),\n",
       " ('clear', 165),\n",
       " ('w', 164),\n",
       " ('therefore', 164),\n",
       " ('suppose', 164),\n",
       " ('sense', 164),\n",
       " ('forth', 164),\n",
       " ('similar', 163),\n",
       " ('reason', 163),\n",
       " ('company', 163),\n",
       " ('dedalus', 162),\n",
       " ('myself', 162),\n",
       " ('indeed', 162),\n",
       " ('foot', 162),\n",
       " ('feet', 162),\n",
       " ('subject', 161),\n",
       " ('received', 161),\n",
       " ('nor', 161),\n",
       " ('afterwards', 161),\n",
       " ('makes', 160),\n",
       " ('interesting', 160),\n",
       " ('bright', 160),\n",
       " ('beginning', 160),\n",
       " ('live', 159),\n",
       " ('military', 159),\n",
       " ('action', 159),\n",
       " ('buckingham', 158),\n",
       " ('la', 158),\n",
       " ('women', 158),\n",
       " ('rest', 158),\n",
       " ('dear', 158),\n",
       " ('character', 158),\n",
       " ('change', 158),\n",
       " ('bridges', 158),\n",
       " ('blue', 158),\n",
       " ('bit', 157),\n",
       " ('greater', 157),\n",
       " ('type', 156),\n",
       " ('table', 156),\n",
       " ('stood', 156),\n",
       " ('pass', 156),\n",
       " ('mean', 156),\n",
       " ('d', 156),\n",
       " ('return', 155),\n",
       " ('note', 155),\n",
       " ('further', 155),\n",
       " ('william', 154),\n",
       " ('empire', 154),\n",
       " ('duke', 154),\n",
       " ('simple', 153),\n",
       " ('including', 153),\n",
       " ('front', 153),\n",
       " ('alone', 153),\n",
       " ('able', 153),\n",
       " ('irish', 152),\n",
       " ('story', 152),\n",
       " ('really', 152),\n",
       " ('low', 152),\n",
       " ('influence', 152),\n",
       " ('letter', 151),\n",
       " ('length', 151),\n",
       " ('believe', 151),\n",
       " ('mulligan', 150),\n",
       " ('gentleman', 150),\n",
       " ('american', 150),\n",
       " ('today', 150),\n",
       " ('met', 150),\n",
       " ('literary', 150),\n",
       " ('deep', 150),\n",
       " ('view', 149),\n",
       " ('taking', 149),\n",
       " ('run', 149),\n",
       " ('lay', 149),\n",
       " ('beyond', 149),\n",
       " ('africa', 148),\n",
       " ('france', 148),\n",
       " ('charles', 148),\n",
       " ('ordinary', 148),\n",
       " ('hope', 148),\n",
       " ('arms', 148),\n",
       " ('h', 147),\n",
       " ('written', 147),\n",
       " ('walls', 147),\n",
       " ('saying', 147),\n",
       " ('method', 147),\n",
       " ('close', 147),\n",
       " ('member', 146),\n",
       " ('third', 146),\n",
       " ('central', 146),\n",
       " ('wu', 145),\n",
       " ('wait', 145),\n",
       " ('outside', 145),\n",
       " ('moon', 145),\n",
       " ('laws', 145),\n",
       " ('hundred', 145),\n",
       " ('cut', 145),\n",
       " ('birds', 145),\n",
       " ('although', 145),\n",
       " ('feel', 144),\n",
       " ('brown', 143),\n",
       " ('tu', 143),\n",
       " ('section', 143),\n",
       " ('nearly', 143),\n",
       " ('forward', 143),\n",
       " ('window', 142),\n",
       " ('tons', 142),\n",
       " ('seven', 142),\n",
       " ('evidence', 142),\n",
       " ('cause', 142),\n",
       " ('science', 141),\n",
       " ('road', 141),\n",
       " ('crown', 141),\n",
       " ('ago', 141),\n",
       " ('ireland', 140),\n",
       " ('cells', 140),\n",
       " ('stars', 140),\n",
       " ('help', 140),\n",
       " ('below', 140),\n",
       " ('married', 139),\n",
       " ('numerous', 139),\n",
       " ('miles', 139),\n",
       " ('heat', 139),\n",
       " ('attack', 139),\n",
       " ('colour', 138),\n",
       " ('idea', 138),\n",
       " ('e', 138),\n",
       " ('corner', 138),\n",
       " ('joe', 137),\n",
       " ('girders', 137),\n",
       " ('series', 137),\n",
       " ('bad', 137),\n",
       " ('wind', 136),\n",
       " ('private', 136),\n",
       " ('paid', 136),\n",
       " ('ones', 136),\n",
       " ('leave', 136),\n",
       " ('gives', 136),\n",
       " ('yu', 135),\n",
       " ('returned', 135),\n",
       " ('particular', 135),\n",
       " ('master', 135),\n",
       " ('ii', 135),\n",
       " ('effect', 135),\n",
       " ('buildings', 134),\n",
       " ('special', 134),\n",
       " ('size', 134),\n",
       " ('hold', 134),\n",
       " ('besides', 134),\n",
       " ('appears', 134),\n",
       " ('dublin', 133),\n",
       " ('suit', 133),\n",
       " ('remains', 133),\n",
       " ('date', 133),\n",
       " ('cold', 133),\n",
       " ('answered', 133),\n",
       " ('daughter', 132),\n",
       " ('plants', 132),\n",
       " ('letters', 132),\n",
       " ('knowledge', 132),\n",
       " ('importance', 132),\n",
       " ('fall', 132),\n",
       " ('comes', 132),\n",
       " ('agreement', 132),\n",
       " ('upper', 131),\n",
       " ('passing', 131),\n",
       " ('understand', 131),\n",
       " ('district', 131),\n",
       " ('distance', 131),\n",
       " ('total', 130),\n",
       " ('purpose', 130),\n",
       " ('bristol', 129),\n",
       " ('result', 129),\n",
       " ('manner', 129),\n",
       " ('bronze', 129),\n",
       " ('amount', 129),\n",
       " ('weight', 128),\n",
       " ('school', 128),\n",
       " ('original', 128),\n",
       " ('necessary', 128),\n",
       " ('material', 127),\n",
       " ('india', 127),\n",
       " ('horse', 127),\n",
       " ('heavy', 127),\n",
       " ('certainly', 127),\n",
       " ('bring', 127),\n",
       " ('area', 127),\n",
       " ('development', 126),\n",
       " ('james', 125),\n",
       " ('wrote', 125),\n",
       " ('higher', 125),\n",
       " ('greatest', 125),\n",
       " ('fell', 125),\n",
       " ('entered', 125),\n",
       " ('blood', 125),\n",
       " ('soul', 124),\n",
       " ('theory', 124),\n",
       " ('none', 124),\n",
       " ('middle', 124),\n",
       " ('late', 124),\n",
       " ('dry', 124),\n",
       " ('capital', 124),\n",
       " ('girder', 123),\n",
       " ('race', 123),\n",
       " ('county', 123),\n",
       " ('clay', 123),\n",
       " ('brick', 123),\n",
       " ('takes', 123),\n",
       " ('remember', 123),\n",
       " ('population', 123),\n",
       " ('knows', 123),\n",
       " ('f', 123),\n",
       " ('conditions', 123),\n",
       " ('ad', 123),\n",
       " ('shows', 122),\n",
       " ('hardly', 122),\n",
       " ('care', 122),\n",
       " ('attention', 122),\n",
       " ('buck', 121),\n",
       " ('famous', 121),\n",
       " ('earl', 120),\n",
       " ('felt', 120),\n",
       " ('books', 120),\n",
       " ('added', 120),\n",
       " ('york', 119),\n",
       " ('america', 119),\n",
       " ('suddenly', 119),\n",
       " ('rose', 119),\n",
       " ('march', 119),\n",
       " ('lake', 119),\n",
       " ('food', 119),\n",
       " ('evening', 119),\n",
       " ('thousand', 118),\n",
       " ('slowly', 118),\n",
       " ('seemed', 118),\n",
       " ('placed', 118),\n",
       " ('mentioned', 118),\n",
       " ('forces', 118),\n",
       " ('direction', 118),\n",
       " ('martin', 117),\n",
       " ('coast', 117),\n",
       " ('waves', 117),\n",
       " ('lips', 117),\n",
       " ('wish', 117),\n",
       " ('service', 117),\n",
       " ('rise', 117),\n",
       " ('marked', 117),\n",
       " ('hours', 117),\n",
       " ('answer', 117),\n",
       " ('temperature', 116),\n",
       " ('girl', 116),\n",
       " ('george', 116),\n",
       " ('ah', 116),\n",
       " ('play', 116),\n",
       " ('ill', 116),\n",
       " ('hour', 116),\n",
       " ('europe', 116),\n",
       " ('employed', 116),\n",
       " ('drawn', 116),\n",
       " ('advantage', 116),\n",
       " ('structure', 115),\n",
       " ('steel', 115),\n",
       " ('pocket', 115),\n",
       " ('title', 115),\n",
       " ('sort', 115),\n",
       " ('need', 115),\n",
       " ('lead', 115),\n",
       " ('island', 115),\n",
       " ('complete', 115),\n",
       " ('chiefly', 115),\n",
       " ('dog', 114),\n",
       " ('single', 114),\n",
       " ('meet', 114),\n",
       " ('walked', 113),\n",
       " ('colonies', 113),\n",
       " ('seem', 113),\n",
       " ('rich', 113),\n",
       " ('places', 113),\n",
       " ('obtained', 113),\n",
       " ('watch', 112),\n",
       " ('unless', 112),\n",
       " ('sound', 112),\n",
       " ('simply', 112),\n",
       " ('sat', 112),\n",
       " ('produced', 112),\n",
       " ('object', 112),\n",
       " ('figure', 112),\n",
       " ('field', 112),\n",
       " ('wonder', 111),\n",
       " ('college', 111),\n",
       " ('beer', 111),\n",
       " ('speak', 111),\n",
       " ('soldiers', 111),\n",
       " ('raised', 111),\n",
       " ('instance', 111),\n",
       " ('information', 111),\n",
       " ('existence', 111),\n",
       " ('established', 111),\n",
       " ('bank', 111),\n",
       " ('associated', 111),\n",
       " ('electrons', 110),\n",
       " ('papers', 110),\n",
       " ('hot', 110),\n",
       " ('wide', 110),\n",
       " ('pay', 110),\n",
       " ('nine', 110),\n",
       " ('leaves', 110),\n",
       " ('bloody', 109),\n",
       " ('thick', 109),\n",
       " ('remarkable', 109),\n",
       " ('process', 109),\n",
       " ('frequently', 109),\n",
       " ('collection', 109),\n",
       " ('beautiful', 109),\n",
       " ('citizen', 108),\n",
       " ('mary', 108),\n",
       " ('top', 108),\n",
       " ('square', 108),\n",
       " ('imperial', 108),\n",
       " ('example', 108),\n",
       " ('carry', 108),\n",
       " ('appeared', 108),\n",
       " ('child', 107),\n",
       " ('showing', 107),\n",
       " ('equal', 107),\n",
       " ('sq', 106),\n",
       " ('sherlock', 106),\n",
       " ('hall', 106),\n",
       " ('truth', 106),\n",
       " ('standing', 106),\n",
       " ('reached', 106),\n",
       " ('party', 106),\n",
       " ('local', 106),\n",
       " ('fresh', 106),\n",
       " ('founded', 106),\n",
       " ('doing', 106),\n",
       " ('battle', 106),\n",
       " ('atoms', 105),\n",
       " ('save', 105),\n",
       " ('origin', 105),\n",
       " ('opened', 105),\n",
       " ('waiting', 105),\n",
       " ('text', 105),\n",
       " ('rule', 105),\n",
       " ('appointed', 105),\n",
       " ('species', 104),\n",
       " ('parliament', 104),\n",
       " ('ask', 104),\n",
       " ('study', 104),\n",
       " ('province', 104),\n",
       " ('passage', 104),\n",
       " ('movement', 104),\n",
       " ('language', 104),\n",
       " ('getting', 104),\n",
       " ('fear', 104),\n",
       " ('described', 104),\n",
       " ('becomes', 104),\n",
       " ('zoe', 103),\n",
       " ('houses', 103),\n",
       " ('copper', 103),\n",
       " ('somewhat', 103),\n",
       " ('served', 103),\n",
       " ('plain', 103),\n",
       " ('meaning', 103),\n",
       " ('individual', 103),\n",
       " ('future', 103),\n",
       " ('finally', 103),\n",
       " ('ear', 103),\n",
       " ('discovered', 103),\n",
       " ('lenehan', 102),\n",
       " ('holy', 102),\n",
       " ('yellow', 102),\n",
       " ('space', 102),\n",
       " ('silver', 102),\n",
       " ('shown', 102),\n",
       " ('required', 102),\n",
       " ('provided', 102),\n",
       " ('months', 102),\n",
       " ('hence', 102),\n",
       " ('former', 102),\n",
       " ('cost', 102),\n",
       " ('bird', 102),\n",
       " ('fair', 101),\n",
       " ('boy', 101),\n",
       " ('port', 101),\n",
       " ('piece', 101),\n",
       " ('ought', 101),\n",
       " ('consists', 101),\n",
       " ('sweet', 100),\n",
       " ('running', 100),\n",
       " ('northern', 100),\n",
       " ('height', 100),\n",
       " ('everything', 100),\n",
       " ('easy', 100),\n",
       " ('condition', 100),\n",
       " ('chi', 100),\n",
       " ('charge', 100),\n",
       " ('mammals', 99),\n",
       " ('arch', 99),\n",
       " ('mass', 99),\n",
       " ('grey', 99),\n",
       " ('construction', 99),\n",
       " ('wild', 99),\n",
       " ('music', 99),\n",
       " ('kingdom', 99),\n",
       " ('iii', 99),\n",
       " ('greek', 99),\n",
       " ('else', 99),\n",
       " ('continued', 99),\n",
       " ('children', 99),\n",
       " ('indian', 98),\n",
       " ('glass', 98),\n",
       " ('floor', 98),\n",
       " ('around', 98),\n",
       " ('tsao', 98),\n",
       " ('strange', 98),\n",
       " ('ss', 98),\n",
       " ('spoke', 98),\n",
       " ('national', 98),\n",
       " ('longer', 98),\n",
       " ('contains', 98),\n",
       " ('spanish', 97),\n",
       " ('german', 97),\n",
       " ('worth', 97),\n",
       " ('walk', 97),\n",
       " ('usual', 97),\n",
       " ('mu', 97),\n",
       " ('enemys', 97),\n",
       " ('difficult', 97),\n",
       " ('addition', 97),\n",
       " ('pray', 96),\n",
       " ('nose', 96),\n",
       " ('secret', 96),\n",
       " ('lies', 96),\n",
       " ('group', 96),\n",
       " ('double', 96),\n",
       " ('copy', 96),\n",
       " ('appearance', 96),\n",
       " ('university', 95),\n",
       " ('yourself', 95),\n",
       " ('western', 95),\n",
       " ('support', 95),\n",
       " ('sand', 95),\n",
       " ('practice', 95),\n",
       " ('peace', 95),\n",
       " ('museum', 95),\n",
       " ('lives', 95),\n",
       " ('kings', 95),\n",
       " ('fixed', 95),\n",
       " ('chang', 95),\n",
       " ('simon', 94),\n",
       " ('queen', 94),\n",
       " ('partly', 94),\n",
       " ('teeth', 94),\n",
       " ('star', 94),\n",
       " ('soft', 94),\n",
       " ('real', 94),\n",
       " ('named', 94),\n",
       " ('increased', 94),\n",
       " ('degree', 94),\n",
       " ('spain', 93),\n",
       " ('shore', 93),\n",
       " ('mortar', 93),\n",
       " ('victory', 93),\n",
       " ('speech', 93),\n",
       " ('portion', 93),\n",
       " ('minutes', 93),\n",
       " ('license', 93),\n",
       " ('kept', 93),\n",
       " ('holding', 93),\n",
       " ('greatly', 93),\n",
       " ('containing', 93),\n",
       " ('chinese', 93),\n",
       " ('brother', 93),\n",
       " ('societies', 92),\n",
       " ('june', 92),\n",
       " ('ways', 92),\n",
       " ('twice', 92),\n",
       " ('sign', 92),\n",
       " ('remained', 92),\n",
       " ('quickly', 92),\n",
       " ('oh', 92),\n",
       " ('giving', 92),\n",
       " ('friends', 92),\n",
       " ('pressure', 91),\n",
       " ('trees', 91),\n",
       " ('throughout', 91),\n",
       " ('succeeded', 91),\n",
       " ('popular', 91),\n",
       " ('larger', 91),\n",
       " ('donations', 91),\n",
       " ('cross', 91),\n",
       " ('mans', 90),\n",
       " ('erected', 90),\n",
       " ('wood', 90),\n",
       " ('sight', 90),\n",
       " ('particularly', 90),\n",
       " ('level', 90),\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob, itertools, operator\n",
    "from multiprocessing import Pool\n",
    "files = glob.glob('hadoop/books/*.txt')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(processes=6) as p:\n",
    "        mapped_values = p.map(words_mp,files)\n",
    "        partitioned_data = partition_mp(itertools.chain(*mapped_values))\n",
    "        reduced_values = p.map(reduce_mp, partitioned_data)\n",
    "        reduced_values.sort(key=operator.itemgetter(1)) # sort values by number of occurences\n",
    "        reduced_values.reverse() # Put highest number of occurences on top\n",
    "        \n",
    "reduced_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [itertools.chain(*mapped_values)](https://docs.python.org/3.6/library/itertools.html#itertools.chain) is used for treating consecutive sequences as a single sequence. \n",
    "- [operator](https://docs.python.org/3/library/operator.html).itemgetter(1)\n",
    "Return a callable object that fetches item from its operand using the operandâ€™s __getitem__() method. \n",
    "```python\n",
    "inventory = [('apple', 3), ('banana', 2), ('pear', 5), ('orange', 1)]\n",
    "getcount = itemgetter(1)\n",
    ">>> list(map(getcount, inventory))\n",
    "[3, 2, 5, 1]\n",
    ">>> sorted(inventory, key=getcount)\n",
    "[('orange', 1), ('banana', 2), ('apple', 3), ('pear', 5)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# sharedmem\n",
    "\n",
    "- http://github.com/rainwoodman/sharedmem\n",
    "- https://pypi.python.org/pypi/sharedmem\n",
    "\n",
    "Create a list of sample texts in different files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lorem import text\n",
    "\n",
    "for i in range(10):\n",
    "    with open(\"hadoop/sample_{0:1d}.txt\".format(i), \"w\") as sample:\n",
    "        sample.write(text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop/sample_0.txt\n",
      "hadoop/sample_1.txt\n",
      "hadoop/sample_2.txt\n",
      "hadoop/sample_7.txt\n",
      "hadoop/sample_3.txt\n",
      "hadoop/sample_8.txt\n",
      "hadoop/sample_4.txt\n",
      "hadoop/sample_5.txt\n",
      "hadoop/sample_9.txt\n",
      "hadoop/sample_6.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dolorem': 11, 'ipsum': 20}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sharedmem\n",
    "import glob\n",
    "samples = glob.glob('hadoop/sample_*.txt')\n",
    "\n",
    "word_count = {\n",
    "        'dolorem': 0,\n",
    "        'ipsum': 0,\n",
    "            }\n",
    "\n",
    "with sharedmem.MapReduce() as pool:\n",
    "\n",
    "    def work(file):\n",
    "        print(file)\n",
    "        with open(file) as f:\n",
    "            for line in f:\n",
    "                my_word_count = dict([(word, 0) for word in word_count])\n",
    "                for word in line.replace('.', ' ').split():\n",
    "                    if word in word_count:\n",
    "                        my_word_count[word] += 1\n",
    "\n",
    "        return my_word_count\n",
    "\n",
    "    def reduce(her_word_count):\n",
    "        for word in word_count:\n",
    "            word_count[word] += her_word_count[word]\n",
    "\n",
    "    pool.map(work, samples, reduce=reduce)\n",
    "\n",
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Deploying the MapReduce code on Hadoop\n",
    "\n",
    "This Python must use the [Hadoop Streaming API](http://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) to pass data between our Map and Reduce code via Pythonâ€™s sys.stdin (standard input) and sys.stdout (standard output). \n",
    "\n",
    "Download some books\n",
    "* [The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson](http://www.gutenberg.org/ebooks/20417.txt.utf-8)\n",
    "* [Ulysses by James Joyce](http://www.gutenberg.org/files/4300/4300-0.txt)\n",
    "* [The Art of War by 6th cent. B.C. Sunzi](http://www.gutenberg.org/ebooks/132.txt.utf-8)\n",
    "* [The Adventures of Sherlock Holmes by Sir Arthur Conan Doyle](http://www.gutenberg.org/ebooks/1661.txt.utf-8)\n",
    "* [The Devilâ€™s Dictionary by Ambrose Bierce](http://www.gutenberg.org/ebooks/972.txt.utf-8)\n",
    "* [Encyclopaedia Britannica, 11th Edition, Volume 4, Part 3](http://www.gutenberg.org/ebooks/19699.txt.utf-8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Map \n",
    "\n",
    "The following Python code read data from sys.stdin, split it into words and output a list of lines mapping words to their (intermediate) counts to sys.stdout. For every word it outputs <word> 1 tuples immediately. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop/mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file hadoop/mapper.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import sys, string\n",
    "\n",
    "# input comes from standard input\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # strip punctuation\n",
    "    line = line.translate(None,string.punctuation)    \n",
    "\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word in words:\n",
    "        # write the results to standard output;\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print ('%s\\t%s' % (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!chmod +x hadoop/mapper.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reduce \n",
    "\n",
    "The following code reads the results of mapper.py and sum the occurrences of each word to a final count, and then output its results to sys.stdout.\n",
    "Remember that Hadoop sorts map output so it is easier to count words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop/reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file hadoop/reducer.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input lines\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to sys.stdout\n",
    "            print ('{}\\t{}'.format(current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print ('{}\\t{}'.format(current_word, current_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!chmod +x hadoop/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amet\t1\r\n",
      "Consectetur\t1\r\n",
      "Dolor\t1\r\n",
      "Dolore\t1\r\n",
      "Dolorem\t2\r\n",
      "Eius\t1\r\n",
      "Etincidunt\t1\r\n",
      "Ipsum\t2\r\n",
      "Labore\t1\r\n",
      "Magnam\t1\r\n",
      "Modi\t1\r\n",
      "Non\t1\r\n",
      "Numquam\t1\r\n",
      "Quaerat\t3\r\n",
      "Quiquia\t1\r\n",
      "Quisquam\t1\r\n",
      "Sed\t2\r\n",
      "Sit\t2\r\n",
      "Tempora\t1\r\n",
      "Voluptatem\t1\r\n",
      "adipisci\t9\r\n",
      "aliquam\t3\r\n",
      "amet\t3\r\n",
      "consectetur\t1\r\n",
      "dolor\t8\r\n",
      "dolore\t10\r\n",
      "dolorem\t7\r\n",
      "eius\t6\r\n",
      "est\t4\r\n",
      "etincidunt\t5\r\n",
      "ipsum\t2\r\n",
      "labore\t3\r\n",
      "magnam\t5\r\n",
      "modi\t3\r\n",
      "neque\t4\r\n",
      "non\t4\r\n",
      "numquam\t4\r\n",
      "porro\t4\r\n",
      "quaerat\t1\r\n",
      "quiquia\t4\r\n",
      "quisquam\t3\r\n",
      "sed\t6\r\n",
      "sit\t5\r\n",
      "tempora\t4\r\n",
      "ut\t6\r\n",
      "velit\t5\r\n",
      "voluptatem\t10\r\n"
     ]
    }
   ],
   "source": [
    "!cat hadoop/sample.txt | ./hadoop/mapper.py | sort | ./hadoop/reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Execution on Hadoop cluster\n",
    "\n",
    "* Copy books to HDFS\n",
    "* Run the WordCount MapReduce\n",
    "\n",
    "Makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hadoop/Makefile\n"
     ]
    }
   ],
   "source": [
    "%%file hadoop/Makefile\n",
    "HADOOP_TOOLS=/usr/local/Cellar/hadoop/2.8.0/libexec/share/hadoop/tools/lib/\n",
    "HDFS_DIR=/user/${USER}\n",
    "\n",
    "BOOKS = books/20417.txt books/5000-8.txt books/4300-0.txt books/132.txt ht books/1661.txt h books/972.txt ht books/19699.txt \n",
    "\n",
    "download:\n",
    "\tmkdir -p books\n",
    "\twget -q -O books/20417.txt http://www.gutenberg.org/ebooks/20417.txt.utf-8\n",
    "\twget -q -O books/4300-0.txt http://www.gutenberg.org/files/4300/4300-0.txt\n",
    "\twget -q -O books/132.txt http://www.gutenberg.org/ebooks/132.txt.utf-8\n",
    "\twget -q -O books/1661.txt http://www.gutenberg.org/ebooks/1661.txt.utf-8\n",
    "\twget -q -O books/972.txt http://www.gutenberg.org/ebooks/972.txt.utf-8\n",
    "\twget -q -O books/19699.txt http://www.gutenberg.org/ebooks/19699.txt.utf-8\n",
    "\n",
    "copy_to_hdfs: ${BOOKS}\n",
    "\thdfs dfs -put books books\n",
    "\t\n",
    "run_with_hadoop: \n",
    "\thadoop jar ${HADOOP_TOOLS}/hadoop-streaming-2.8.0.jar \\\n",
    "    -file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
    "    -file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
    "    -input ${HDFS_DIR}/books/* -output ${HDFS_DIR}/output-hadoop\n",
    "\n",
    "run_with_yarn: \n",
    "\tyarn jar ${HADOOP_TOOLS}/hadoop-streaming-2.8.0.jar \\\n",
    "\t-file  ${PWD}/mapper.py  -mapper  ${PWD}/mapper.py \\\n",
    "\t-file  ${PWD}/reducer.py -reducer ${PWD}/reducer.py \\\n",
    "\t-input ${HDFS_DIR}/books/* -output ${HDFS_DIR}/output-yarn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Run\n",
    "\n",
    "```bash\n",
    "$ make run_with_hadoop\n",
    "\n",
    "$ make run_with_yarn\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
