{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierre Navaro - [Institut de Recherche Mathématique de Rennes](https://irmar.univ-rennes1.fr) - [CNRS](http://www.cnrs.fr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* [multiprocessing basics](https://pymotw.com/2/multiprocessing/basics.html)\n",
    "* [Python 201: A multiprocessing tutorial](https://www.blog.pythonlibrary.org/2016/08/02/python-201-a-multiprocessing-tutorial/)\n",
    "* [Multithread - Konrad Hinsen](http://calcul.math.cnrs.fr/Documents/Ecoles/2013/python/Multiprocessing.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multiprocessing\n",
    "\n",
    "- The multiprocessing allows the programmer to fully leverage multiple processors. \n",
    "- It runs on both Unix and Windows.\n",
    "- The `Pool` object parallelizes the execution of a function across multiple input values.\n",
    "- The if `__name__ == '__main__'` part is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 5, 10, 17, 26, 37, 50]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def f(x): return x*x+1  # Function executed on worker processes.\n",
    "\n",
    "if __name__ == '__main__': # Executed only on main process.\n",
    "    with Pool(4) as p:\n",
    "        print(p.map(f, list(range(8))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 10, 12, 14]\n"
     ]
    }
   ],
   "source": [
    "def g(x, y): return x+y  # Function executed on worker processes.\n",
    "\n",
    "if __name__ == '__main__': # Executed only on main process.\n",
    "    with Pool(4) as p:\n",
    "        print(p.starmap(g, \n",
    "            ((x,y) for x,y in zip(list(range(8)),list(range(8))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 1.4142135623730951, 1.7320508075688772, 2.0, 2.2360679774997898, 2.4494897427831779, 2.6457513110645907, 2.8284271247461903, 3.0]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import numpy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pool = Pool()\n",
    "    results = [pool.apply_async(numpy.sqrt, (x,))\n",
    "               for x in range(10)]\n",
    "    roots = [r.get() for r in results]\n",
    "    print(roots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use for:\n",
    "- `pool.apply_async` returns a proxy object immediately\n",
    "- `proxy.get()` waits for task completion and returns the result\n",
    "- launching different tasks in parallel\n",
    "- launching tasks with more than one argument \n",
    "- better control of task distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Process class\n",
    "\n",
    "In multiprocessing, processes are spawned by creating a Process object and then calling its start() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello bob\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "def f(name):\n",
    "    print('hello', name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = Process(target=f, args=('bob',))\n",
    "    p.start()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contexts and start methods\n",
    "Depending on the platform, multiprocessing supports \n",
    "three ways to start a process:\n",
    "- *spawn*: The parent process starts a fresh python interpreter process. Unix and Windows. The default on Windows.\n",
    "- *fork*: The child process, when it begins, is effectively identical to the parent process. All resources of the parent are inherited by the child process. Available on Unix only. The default on Unix.\n",
    "- *forkserver*: A server process is started. When a new process is needed, the parent process connects to the server and requests that it fork a new process. Available on Unix.\n",
    "\n",
    "To select a start method you use the `set_start_method()` in the if `__name__ == '__main__'` clause of the main module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Exchanging objects between processes\n",
    "\n",
    "## Queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, None, 'hello']\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def f(q):\n",
    "    q.put([42, None, 'hello'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    q = Queue()\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    print(q.get())    # prints \"[42, None, 'hello']\"\n",
    "    p.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipes\n",
    "Pipe() returns two connection objects. Each connection object has send() and recv() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, None, 'hello']\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "\n",
    "def f(conn):\n",
    "    conn.send([42, None, 'hello'])\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parent_conn, child_conn = Pipe()\n",
    "    p = Process(target=f, args=(child_conn,))\n",
    "    p.start()\n",
    "    print(parent_conn.recv())   # prints \"[42, None, 'hello']\"\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Synchronization between processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 "
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Lock\n",
    "\n",
    "def f(l, i):\n",
    "    l.acquire()\n",
    "    try:\n",
    "        print( i, end = ' ')\n",
    "    finally:\n",
    "        l.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lock = Lock()\n",
    "\n",
    "    for num in range(10):\n",
    "        Process(target=f, args=(lock, num)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 "
     ]
    }
   ],
   "source": [
    "def f( i):\n",
    "    print( i, end = ' ')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for num in range(10):\n",
    "        Process(target=f, args=(num,)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Shared memory between processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1415927\n",
      "[0, -1, -2, -3, -4, -5, -6, -7, -8, -9]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Value, Array\n",
    "\n",
    "def f(n, a):\n",
    "    n.value = 3.1415927\n",
    "    for i in range(len(a)):\n",
    "        a[i] = -a[i]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num = Value('d', 0.0)\n",
    "    arr = Array('i', range(10))\n",
    "\n",
    "    p = Process(target=f, args=(num, arr))\n",
    "    p.start()\n",
    "    p.join()\n",
    "\n",
    "    print(num.value)\n",
    "    print(arr[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This eliminates the serialization overhead.\n",
    "- A NumPy extension adds shared NumPy arrays (numpy-sharedmem).\n",
    "- Don’t modify shared memory contents in the slave processes. \n",
    "- Use shared memory only to transfer data from the master to the slaves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Internal Numpy error: too many arguments in call to PyUFunc_HasOverride",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\nTypeError: Internal Numpy error: too many arguments in call to PyUFunc_HasOverride\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-af5c3a5bcdfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Internal Numpy error: too many arguments in call to PyUFunc_HasOverride"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "import sharedmem\n",
    "\n",
    "def distribute(nitems, nprocs):\n",
    "    \"\"\" Distributes a sequence equally (as much as possible) \n",
    "    over the available processors. Returns a list of index pairs \n",
    "    (imin, imax) that delimit the slice to give to one task.\"\"\"\n",
    "    nitems_per_proc = (nitems+nprocs-1)//nprocs\n",
    "    return [(i, min(nitems, i+nitems_per_proc))\n",
    "            for i in range(0, nitems, nitems_per_proc)]\n",
    "\n",
    "def apply_sqrt(a, imin, imax):\n",
    "    return np.sqrt(a[imin:imax])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nprocs = 4\n",
    "    \n",
    "    data = sharedmem.empty((100,), np.float)\n",
    "    data[:] = np.arange(100)\n",
    "    with Pool(processes=nprocs) as pool:\n",
    "        \n",
    "        \n",
    "        slices = distribute(len(data), nprocs)\n",
    "        result = pool.apply_async(np.sqrt, data)\n",
    "        print(result.get(timeout=1))\n",
    "       \n",
    "        \n",
    "        #\n",
    "        \n",
    "        #results = [pool.apply_async(apply_sqrt, (data, imin, imax))\n",
    "        #           for (imin, imax) in slices]\n",
    "        #for r, (imin, imax) in zip(results, slices):\n",
    "        #    data[imin:imax] = r.get()\n",
    "    print(slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel Pi calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores 1, Estimated value of Pi : 3.14164440 time : 3.61585712\n",
      "Number of cores 2, Estimated value of Pi : 3.14194720 time : 1.68908000\n",
      "Number of cores 3, Estimated value of Pi : 3.14078200 time : 1.13751817\n",
      "Number of cores 4, Estimated value of Pi : 3.14018080 time : 0.90940595\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def compute_pi(n):\n",
    "    count = 0\n",
    "    for i in range(n):\n",
    "        x=random.random()\n",
    "        y=random.random()\n",
    "        if x*x + y*y <= 1: count+=1\n",
    "    return count\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    \n",
    "    for np in range(1,5):\n",
    "        elapsed_time = time.time()\n",
    "        assert ( np <= cpu_count())\n",
    "        \n",
    "        n = 10000000\n",
    "        part_count=[n//np for i in range(np)]\n",
    "        pool = Pool(processes=np)   \n",
    "        count=pool.map(compute_pi, part_count)\n",
    "        print (\"Number of cores {0}, Estimated value of Pi : {1:.8f}\"\n",
    "       \" time : {2:.8f}\".format(np, 4*sum(count)/n,time.time()-elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Joblib\n",
    "\n",
    "[Joblib](http://pythonhosted.org/joblib/) provides a simple helper class to write parallel for loops using multiprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "[f(x) for x in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "Parallel(n_jobs=2)(delayed(f)(x) for x in range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercice\n",
    "\n",
    "Write the program in which two processes send packets of information back and forth a 100 times and record the amount of time required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 doubled to 10 by: Process-2\n",
      "10 doubled to 20 by: Process-3\n",
      "15 doubled to 30 by: Process-4\n",
      "20 doubled to 40 by: Process-5\n",
      "25 doubled to 50 by: Process-6\n",
      "2 doubled to 4 by: Test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "from multiprocessing import Process, current_process\n",
    " \n",
    " \n",
    "def doubler(number):\n",
    "    \"\"\"\n",
    "    A doubling function that can be used by a process\n",
    "    \"\"\"\n",
    "    result = number * 2\n",
    "    proc_name = current_process().name\n",
    "    print('{0} doubled to {1} by: {2}'.format(\n",
    "        number, result, proc_name))\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    numbers = [5, 10, 15, 20, 25]\n",
    "    procs = []\n",
    "    proc = Process(target=doubler, args=(5,))\n",
    " \n",
    "    for index, number in enumerate(numbers):\n",
    "        proc = Process(target=doubler, args=(number,))\n",
    "        procs.append(proc)\n",
    "        proc.start()\n",
    " \n",
    "    proc = Process(target=doubler, name='Test', args=(2,))\n",
    "    proc.start()\n",
    "    procs.append(proc)\n",
    " \n",
    "    for proc in procs:\n",
    "        proc.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# sharedmem\n",
    "\n",
    "http://rainwoodman.github.io/sharedmem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 45.]\n"
     ]
    }
   ],
   "source": [
    "import sharedmem\n",
    "counter = sharedmem.empty(1)\n",
    "counter[:] = 0\n",
    "with sharedmem.MapReduce() as pool:\n",
    "    def work(i):\n",
    "         with pool.critical:\n",
    "             counter[:] += i\n",
    "    pool.map(work, range(10))\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0 done\n",
      "chunk 1048576 done\n",
      "chunk 2097152 done\n",
      "chunk 3145728 done\n",
      "chunk 4194304 done\n",
      "chunk 5242880 done\n",
      "chunk 6291456 done\n",
      "chunk 7340032 done\n",
      "chunk 8388608 done\n",
      "chunk 9437184 done\n",
      "chunk 10485760 done\n",
      "chunk 11534336 done\n",
      "chunk 12582912 done\n",
      "chunk 13631488 done\n",
      "chunk 14680064 done\n",
      "chunk 15728640 done\n",
      "chunk 16777216 done\n",
      "chunk 17825792 done\n",
      "chunk 18874368 done\n",
      "chunk 19922944 done\n",
      "chunk 24117248 done\n",
      "chunk 20971520 done\n",
      "chunk 22020096 done\n",
      "chunk 23068672 done\n",
      "chunk 25165824 done\n",
      "chunk 26214400 done\n",
      "chunk 27262976 done\n",
      "chunk 29360128 done\n",
      "chunk 28311552 done\n",
      "chunk 33554432 done\n",
      "chunk 30408704 done\n",
      "chunk 31457280 done\n",
      "chunk 32505856 done\n",
      "chunk 34603008 done\n",
      "chunk 36700160 done\n",
      "chunk 35651584 done\n",
      "chunk 37748736 done\n",
      "chunk 39845888 done\n",
      "chunk 38797312 done\n",
      "chunk 40894464 done\n",
      "chunk 41943040 done\n",
      "chunk 42991616 done\n",
      "chunk 44040192 done\n",
      "chunk 50331648 done\n",
      "chunk 45088768 done\n",
      "chunk 46137344 done\n",
      "chunk 47185920 done\n",
      "chunk 49283072 done\n",
      "chunk 48234496 done\n",
      "chunk 51380224 done\n",
      "chunk 52428800 done\n",
      "chunk 53477376 done\n",
      "chunk 54525952 done\n",
      "chunk 55574528 done\n",
      "chunk 57671680 done\n",
      "chunk 58720256 done\n",
      "chunk 56623104 done\n",
      "chunk 59768832 done\n",
      "chunk 60817408 done\n",
      "chunk 61865984 done\n",
      "chunk 62914560 done\n",
      "chunk 67108864 done\n",
      "chunk 63963136 done\n",
      "chunk 68157440 done\n",
      "chunk 65011712 done\n",
      "chunk 66060288 done\n",
      "chunk 69206016 done\n",
      "chunk 71303168 done\n",
      "chunk 70254592 done\n",
      "chunk 73400320 done\n",
      "chunk 72351744 done\n",
      "chunk 74448896 done\n",
      "chunk 75497472 done\n",
      "chunk 76546048 done\n",
      "chunk 77594624 done\n",
      "chunk 80740352 done\n",
      "chunk 82837504 done\n",
      "chunk 78643200 done\n",
      "chunk 83886080 done\n",
      "chunk 84934656 done\n",
      "chunk 81788928 done\n",
      "chunk 85983232 done\n",
      "chunk 79691776 done\n",
      "chunk 88080384 done\n",
      "chunk 87031808 done\n",
      "chunk 89128960 done\n",
      "chunk 90177536 done\n",
      "chunk 91226112 done\n",
      "chunk 92274688 done\n",
      "chunk 93323264 done\n",
      "chunk 94371840 done\n",
      "chunk 100663296 done\n",
      "chunk 101711872 done\n",
      "chunk 102760448 done\n",
      "chunk 96468992 done\n",
      "chunk 97517568 done\n",
      "chunk 95420416 done\n",
      "chunk 99614720 done\n",
      "chunk 98566144 done\n",
      "chunk 103809024 done\n",
      "chunk 105906176 done\n",
      "chunk 104857600 done\n",
      "chunk 106954752 done\n",
      "chunk 108003328 done\n",
      "chunk 110100480 done\n",
      "chunk 109051904 done\n",
      "chunk 111149056 done\n",
      "chunk 112197632 done\n",
      "chunk 118489088 done\n",
      "chunk 117440512 done\n",
      "chunk 119537664 done\n",
      "chunk 113246208 done\n",
      "chunk 114294784 done\n",
      "chunk 115343360 done\n",
      "chunk 116391936 done\n",
      "chunk 120586240 done\n",
      "chunk 121634816 done\n",
      "chunk 123731968 done\n",
      "chunk 122683392 done\n",
      "chunk 125829120 done\n",
      "chunk 124780544 done\n",
      "chunk 126877696 done\n",
      "chunk 127926272 done\n",
      "chunk 128974848 done\n",
      "chunk 131072000 done\n",
      "chunk 130023424 done\n",
      "chunk 132120576 done\n",
      "chunk 133169152 done\n",
      "9.00719918763e+15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input = np.arange(1024 * 1024 * 128, dtype='f8')\n",
    "output = sharedmem.empty(1024 * 1024 * 128, dtype='f8')\n",
    "with sharedmem.MapReduce() as pool:\n",
    "    chunksize = 1024 * 1024\n",
    "    def work(i):\n",
    "        s = slice (i, i + chunksize)\n",
    "        output[s] = input[s]\n",
    "        return i, np.sum(input[s])\n",
    "    def reduce(i, r):\n",
    "        print('chunk', i, 'done')\n",
    "        return r\n",
    "    r = pool.map(work, range(0, len(input), chunksize), reduce=reduce)\n",
    "print (np.sum(r))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
