{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Pierre Navaro - [Institut de Recherche MathÃ©matique de Rennes](https://irmar.univ-rennes1.fr) - [CNRS](http://www.cnrs.fr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# References\n",
    "- <a id='hadoop'></a>[Official Apache Hadoop Website](http://hadoop.apache.org/) \n",
    "- [Hadoop Wiki](https://wiki.apache.org/hadoop/FrontPage)\n",
    "- [Outils pour le Big Data - Pierre Nerzic ðŸ‡«ðŸ‡·](https://perso.univ-rennes1.fr/pierre.nerzic/Hadoop/)\n",
    "- [wikistat - Ateliers Big Data - Philippe Besse ðŸ‡«ðŸ‡·](https://github.com/wikistat/Ateliers-Big-Data)\n",
    "- [Big data - Wikipedia](https://en.wikipedia.org/wiki/Big_data)\n",
    "- <a id='hedlung'></a>[Understanding Hadoop Clusters and the Network - Brad Hedlung](http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/)\n",
    "- [Running Hadoop on Ubuntu Linux (Single-Node Cluster)](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/)\n",
    "- [Setting up Hadoop 2.4 and Pig 0.12 on OSX locally](https://getblueshift.com/setting-up-hadoop-2-4-and-pig-0-12-on-osx-locally/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation on macOS Sierra\n",
    "\n",
    "## Java\n",
    "```bash\n",
    "$ java -version\n",
    "java version \"1.8.0_121\"\n",
    "Java(TM) SE Runtime Environment (build 1.8.0_121-b13)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)\n",
    "```\n",
    "\n",
    "## SSH\n",
    "Go to **Preferences Sharing** menu and enable **Remote login**.\n",
    "```bash\n",
    "$ ssh localhost\n",
    "$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "$ exit\n",
    "```\n",
    "Check you can log again without password.\n",
    "\n",
    "## Bash\n",
    "Hadoop put executables in /usr/local/sbin.\n",
    "```bash\n",
    "$ echo 'export PATH=\"/usr/local/sbin:$PATH\"' >> ~/.bash_profile\n",
    "```\n",
    "\n",
    "\n",
    "## Hadoop\n",
    "\n",
    "```bash\n",
    "$ brew install hadoop\n",
    "$ hadoop \n",
    "Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]\n",
    "  CLASSNAME            run the class named CLASSNAME\n",
    " or\n",
    "  where COMMAND is one of:\n",
    "  fs                   run a generic filesystem user client\n",
    "  version              print the version\n",
    "  jar <jar>            run a jar file\n",
    "                       note: please use \"yarn jar\" to launch\n",
    "                             YARN applications, not this command.\n",
    "  checknative [-a|-h]  check native hadoop and compression libraries availability\n",
    "  distcp <srcurl> <desturl> copy file or directories recursively\n",
    "  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive\n",
    "  classpath            prints the class path needed to get the\n",
    "                       Hadoop jar and the required libraries\n",
    "  credential           interact with credential providers\n",
    "  daemonlog            get/set the log level for each daemon\n",
    "  trace                view and modify Hadoop tracing settings\n",
    "\n",
    "Most commands print help when invoked w/o parameters.\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "\n",
    "```bash\n",
    "$ brew info hadoop\n",
    "hadoop: stable 2.8.0\n",
    "Framework for distributed processing of large data sets\n",
    "https://hadoop.apache.org/\n",
    "/usr/local/Cellar/hadoop/2.8.0 (25,169 files, 2.1GB) *\n",
    "  Built from source on 2017-08-15 at 13:36:34\n",
    "From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/hadoop.rb\n",
    "==> Requirements\n",
    "Required: java >= 1.7 âœ”\n",
    "==> Caveats\n",
    "In Hadoop's config file:\n",
    "  /usr/local/opt/hadoop/libexec/etc/hadoop/hadoop-env.sh,\n",
    "  /usr/local/opt/hadoop/libexec/etc/hadoop/mapred-env.sh and\n",
    "  /usr/local/opt/hadoop/libexec/etc/hadoop/yarn-env.sh\n",
    "$JAVA_HOME has been set to be the output of:\n",
    "  /usr/libexec/java_home\n",
    "```\n",
    "\n",
    "* /usr/local/opt/hadoop/libexec/etc/hadoop/core-site.xml\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://localhost:9000</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "* /usr/local/opt/hadoop/libexec/etc/hadoop/hdfs-site.xml\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Map Reduce\n",
    " /usr/local/opt/hadoop/libexec/etc/hadoop/mapred-site.xml\n",
    "```xml\n",
    "<configuration>\n",
    " <property>\n",
    "  <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    " </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "## Yarn\n",
    "/usr/local/opt/hadoop/libexec/etc/hadoop/yarn-site.xml\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big Data\n",
    "\n",
    "- Data sets that are so large or complex that traditional data processing application software is inadequate to deal with them. \n",
    "- Data analysis requires massively parallel software running on several servers.\n",
    "- **Volume, Variety, Velocity, Variability and Veracity** describe Big Data properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Hadoop\n",
    "\n",
    "- Framework for running applications on large cluster. \n",
    "- The Hadoop framework transparently provides applications both reliability and data motion. \n",
    "- Hadoop implements the computational paradigm named **Map/Reduce**, where the application is divided into many small fragments of work, each of which may be executed or re-executed on any node in the cluster. \n",
    "- It provides a distributed file system (HDFS) that stores data on the compute nodes, providing very high aggregate bandwidth across the cluster.\n",
    "- Both MapReduce and the **Hadoop Distributed File System** are designed so that node failures are automatically handled by the framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# HDFS\n",
    "* It is a distributed file systems.\n",
    "* HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware.\n",
    "* HDFS is suitable for applications that have large data sets. \n",
    "* HDFS provides interfaces to move applications closer to where the data is located. The computation is much more efficient when the size of the data set is huge. \n",
    "* HDFS consists of a single NameNode with a number of DataNodes which manage storage. \n",
    "* HDFS exposes a file system namespace and allows user data to be stored in files. \n",
    "    1. A file is split by the NameNode into blocks stored in DataNodes. \n",
    "    2. The **NameNode** executes operations like opening, closing, and renaming files and directories.\n",
    "    3. The **Secondary NameNode** stores information from **NameNode**. \n",
    "    4. The **DataNodes** manage perform block creation, deletion, and replication upon instruction from the NameNode.\n",
    "    5. The placement of replicas is optimized for data reliability, availability, and network bandwidth utilization.\n",
    "    6. User data never flows through the NameNode.\n",
    "* Files in HDFS are write-once and have strictly one writer at any time.\n",
    "* The DataNode has no knowledge about HDFS files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Accessibility dfs\n",
    "\n",
    "All [HDFS commands](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)  are invoked by the bin/hdfs Java script:\n",
    "```shell\n",
    "hdfs [SHELL_OPTIONS] COMMAND [GENERIC_OPTIONS] [COMMAND_OPTIONS]\n",
    "```\n",
    "## Manage files and directories\n",
    "```shell\n",
    "hdfs dfs -ls -h -R # Recursively list subdirectories with human-readable file sizes.\n",
    "hdfs dfs -cp  # Copy files from source to destination\n",
    "hdfs dfs -mv  # Move files from source to destination\n",
    "hdfs dfs -mkdir /foodir # Create a directory named /foodir\t\n",
    "hdfs dfs -rmr /foodir   # Remove a directory named /foodir\t\n",
    "hdfs dfs -cat /foodir/myfile.txt #View the contents of a file named /foodir/myfile.txt\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer between nodes\n",
    "\n",
    "## put\n",
    "```shell\n",
    "hdfs fs -put [-f] [-p] [-l] [-d] [ - | <localsrc1> .. ]. <dst>\n",
    "```\n",
    "Copy single src, or multiple srcs from local file system to the destination file system. \n",
    "\n",
    "Options:\n",
    "\n",
    "    -p : Preserves rights and modification times.\n",
    "    -f : Overwrites the destination if it already exists.\n",
    "\n",
    "```shell\n",
    "hdfs fs -put localfile /user/hadoop/hadoopfile\n",
    "hdfs fs -put -f localfile1 localfile2 /user/hadoop/hadoopdir\n",
    "```\n",
    "Similar to the fs -put command\n",
    "- `moveFromLocal` : to delete the source localsrc after copy.\n",
    "- `copyFromLocal` : source is restricted to a local file\n",
    "- `copyToLocal` : destination is restricted to a local file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![hdfs blocks](http://saphanatutorial.com/wp-content/uploads/2014/06/Hadoop-Course-4.jpg)\n",
    "\n",
    "The Name Node is not in the data path. The Name Node only provides the map of where data is and where data should go in the cluster (file system metadata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Before running Hadoop format HDFS\n",
    "```bash\n",
    "$ hdfs namenode -format\n",
    "```\n",
    "\n",
    "- In output you should have\n",
    "```bash\n",
    "Storage directory /tmp/hadoop-*/dfs/name has been successfully formatted.\n",
    "```\n",
    "\n",
    "- Start NameNode daemon and DataNode daemon:\n",
    "```bash\n",
    "$ start-dfs.sh\n",
    "```\n",
    "You can browse the web interface for NameNode at http://localhost:50070/\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To supress warnings about native-hadoop library do:\n",
    "```bash\n",
    "export HADOOP_HOME_WARN_SUPPRESS=1\n",
    "export HADOOP_ROOT_LOGGER=\"WARN,DRFA\"\n",
    "```\n",
    "- Make the HDFS directories required to execute MapReduce jobs:\n",
    "```bash\n",
    "$ bin/hdfs dfs -mkdir -p /user/$USER\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Log on to the cluster and type the following commands: \n",
    "```bash\n",
    "$ hdfs dfs -ls\n",
    "$ hdfs dfs -ls /\n",
    "$ hdfs dfs -mkdir books\n",
    "```\n",
    "- Create a local file user.txt containing your name and the date:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "$ echo \"Pierre Navaro\" > user.txt\n",
    "$ echo `date` >> user.txt \n",
    "$ cat user.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Copy it on  HDFS :\n",
    "```bash\n",
    "hdfs dfs -put user.txt\n",
    "```\n",
    "\n",
    "Check with:\n",
    "```bash\n",
    "$ hdfs dfs -ls -R \n",
    "$ hdfs dfs -cat user.txt \n",
    "$ hdfs dfs -tail user.txt \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remove the file:\n",
    "```bash\n",
    "$ hdfs dfs -rm user.txt\n",
    "```\n",
    "\n",
    "Put it again on HDFS and move to books directory:\n",
    "```bash\n",
    "$ hdfs dfs -copyFromLocal user.txt\n",
    "$ hdfs dfs -mv user.txt books/user.txt\n",
    "$ hdfs dfs -ls -R -h\n",
    "```\n",
    "\n",
    "Copy user.txt to hello.txt and remove it.\n",
    "```bash\n",
    "$ hdfs dfs -cp books/user.txt books/hello.txt\n",
    "$ hdfs dfs -count -h /user/$USER\n",
    "$ hdfs dfs -rm books/user.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Run MapReduce example job.\n",
    "\n",
    "```bash\n",
    "$ mkdir input\n",
    "$ cp /usr/local/opt/hadoop/libexec/etc/hadoop/*.xml input/\n",
    "$ hdfs dfs -put input\n",
    "$ hadoop jar \\\n",
    "  /usr/local/opt/hadoop/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.0.jar grep input output 'dfs[a-z.]+'\n",
    "$ hdfs dfs -ls output\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# YARN\n",
    "\n",
    "Start NameNode daemon and DataNode daemon:\n",
    "```bash\n",
    "$ start-yarn.sh\n",
    "```\n",
    "You can browse two more web interfaces:\n",
    "   - JobTracker: http://localhost:8088/\n",
    "   - Node Specific Info: http://localhost:8042/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WordCount Example ([hadoop wiki](https://wiki.apache.org/hadoop/WordCount))\n",
    "\n",
    "Download three ebooks from Project Gutenberg as input data.\n",
    "- The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson\n",
    "- The Notebooks of Leonardo Da Vinci â€” Complete by da Vinci Leonardo\n",
    "- Ulysses by James Joyce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```bash\n",
    "$ mkdir -p books\n",
    "$ wget -q -O books/20417.txt  http://www.gutenberg.org/ebooks/20417.txt.utf-8\n",
    "$ wget -q -O books/5000-8.txt http://www.gutenberg.org/files/5000/5000-8.txt\n",
    "$ wget -q -O books/4300-0.txt http://www.gutenberg.org/files/4300/4300-0.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20417.txt   4300-0.txt  5000-8.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
